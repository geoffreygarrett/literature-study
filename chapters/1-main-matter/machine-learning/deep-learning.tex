\section{Deep Learning\label{sec:DL}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\Gls{DL} is a field of \gls{ML} that is primarily concerned with the learning of
representations of data. The main difference between a \gls{NN} and a
traditional \gls{ML} algorithm is that the neural network is a function of the
data, rather than the data itself.

\subsection{Perceptrons}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Proposed by Rosenblatt \cite{Rosenblatt_1957_6098} in his technical
report funded by the United States Office of Naval Research
\cite{doi:10.1177/030631296026003005} in 1957, the \textit{perceptron} is a
fundamental component of deep learning, describing a mathematical model of a
biological neuron. There are two different sets of notation which exist when
dealing with the bias of a perceptron. One involves the inclusion of a unit
constant in the input vector $\mathbf{x}$, with the bias being specified by the
value of the first weight $w_0$. An alternative notation exists which treats the
bias as a standalone value $b$. The latter notation will be used, as it is the
preferred notation in contemporary deep learning research. The notation defining
the mapping of a perceptron is then: $f(\gls{x_in};\gls{w_vec},
\gls{b})=\gls{x_in}^T\gls{w_vec}+\gls{b}$; $\gls{x_in}\in\gls{set:R}^{\gls{n_x}}$,
$\gls{w_vec}\in\gls{set:R}^{\gls{n_x}}$, $\gls{b}\in\gls{set:R}$.

\begin{figure}[htbp]
    \centering
    \input{graphics/tikz/perceptron}
    \caption{Perceptron}
    \label{fig:perceptron}
\end{figure}

This mathematical mimicry of a biological neuron was first used proposed by
Rosenblatt where a set of inputs $\mathbf{x}$ weighted using $\mathbf{w}$ before
being summed. If this sum surpassed a threshold, then a binary output of $1$ was
returned. \autoref{fig:perceptron} shows a more general formulation which
applies to the resulting field of deep learning, with any activation function
$\phi$ analogous for the level of excitation of a biological neuron in response
to its stimulus $\mathbf{x}$.

The primary criticism of the perceptron came in 1969 from Minsky and Papert
\cite{minsky69perceptrons}, where it was shown that the perceptron could only
solve \textit{linearly separable} functions, and failed to solve the XOR and
NXOR functions. They went on to claim that the research being done was doomed to
failure due to these limitations, resulting in little research in the area being
done until about the 1980's.

\subsection{Activation function}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Since the early days of the perceptron, a wide variety of activation functions
$\phi$ have been used and improve upon the threshold step function.

%\begin{figure}[htbp]
%    \centering
%    \input{graphics/tikz/activation}
%    \caption{Activation functions}
%    \label{fig:activation}
%\end{figure}
\input{graphics/tikz/activation}

\subsection{Multi-layer perceptrons}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Gls{MLPs}, otherwise known as \textbf{artificial neural networks} or
\textbf{feedforward neural networks}, are the fundamental models of \gls{DL}.
They adapt and

\textbf{Deep feedforward networks}
\\
$f(\gls{x_in}; \gls{theta})$; $\gls{x_in}\in\mathbb{R}^{\gls{n_x}}$

\begin{figure}
    \centering
    \input{graphics/tikz/mlp}
    \caption{Multilayer perceptron}
    \label{fig:mlp}
\end{figure}


\begin{figure}
    \centering
    \input{graphics/tikz/mlp-vec}
    \caption{Multilayer perceptron}
    \label{fig:mlp-vec}
\end{figure}



\begin{equation}
    \mathbf{a}^{[1]} = g^{[1]} \bigg(\gls{dl:W:1}^T\gls{x_in} + \mathbf{b}^{[1]}\bigg);
\end{equation}

\begin{equation}
    \mathbf{a}^{[2]} = g^{[2]} \bigg(\gls{dl:W:2}^T\mathbf{h}^{[2]} + \mathbf{b}^{[2]}\bigg);
\end{equation}

\gls{dl:W:l}

\subsection{Gradient based learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Backpropagation, short for "backward propagation of errors," is a
\textit{supervised learning} algorithm for artificial neural networks using
\textit{gradient descent}.

\begin{figure}[htp]
    \centering
    \input{graphics/tikz/gradient-descent}
    \caption{Gradient based learning}
    \label{fig:gradient-descent}
\end{figure}


\begin{figure}[htp]
    \centering
    \input{graphics/tikz/gradient-descent-3d}
    \caption{Gradient based learning}
    \label{fig:gradient-descent}
\end{figure}

\subsection{Universal Approximation Properties and Depth}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A linear model by definition, may only optimised to represent linear functions.
It has advantages in its simplicity to optimise however we often require our
estimator models to learn nonlinear functions.

%\subsection{Backpropagation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


