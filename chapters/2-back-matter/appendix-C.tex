\newpage\section{Machine Learning}

\subsection{Perceptron logic gates}


\begin{figure}[!htp]
    \centering
    \begin{subfigure}[b]{0.70\textwidth}
        \centering
        \input{graphics/tikz/perceptron_not}
        \captionsetup{format=hang} % hanging captions
        \subcaption{Perceptron mapping}
        \label{fig:perceptron:not:mapping}
    \end{subfigure}\hfil
    \begin{subfigure}[b]{0.29\textwidth}
        \centering
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{C{0.1\linewidth}|C{0.1\linewidth}}
            \hline
            $x_1$ & $y$ \\ \hline
            0     & 1   \\
            1     & 0
        \end{tabular}
        \vspace{0.5cm}
        \subcaption{Truth table}
        \label{fig:perceptron:not:truth}
    \end{subfigure}\hfil
    \captionsetup{format=hang} % hanging captions
    \caption{
            \textbf{Perceptron mapping for the NOT logic gate.}
    }
    \label{fig:perceptron:not}
\end{figure}

\begin{figure}[!htp]
    \centering
    \begin{subfigure}[b]{0.70\textwidth}
        \centering
        \input{graphics/tikz/perceptron_nor}
        \captionsetup{format=hang} % hanging captions
        \subcaption{Perceptron mapping}
        \label{fig:perceptron:nor:mapping}
    \end{subfigure}\hfil
    \begin{subfigure}[b]{0.29\textwidth}
        \centering
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{C{0.1\linewidth}C{0.1\linewidth}|C{0.1\linewidth}}
            \hline
            $x_1$ & $x_1$ & $y$ \\ \hline
            0     & 0     & 1   \\
            0     & 1     & 0   \\
            1     & 0     & 0   \\
            1     & 1     & 0
        \end{tabular}
        \vspace{0.5cm}
        \subcaption{Truth table}
        \label{fig:perceptron:nor:truth}
    \end{subfigure}\hfil
    \captionsetup{format=hang} % hanging captions
    \caption{
        \textbf{Perceptron mapping for the logical NOR gate.}
    }
    \label{fig:perceptron:nor}
\end{figure}

\begin{figure}[!htp]
    \centering
    \begin{subfigure}[b]{0.70\textwidth}
        \centering
        \input{graphics/tikz/perceptron_or}
        \captionsetup{format=hang} % hanging captions
        \subcaption{Perceptron mapping}
        \label{fig:perceptron:or:mapping}
    \end{subfigure}\hfil
    \begin{subfigure}[b]{0.29\textwidth}
        \centering
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{C{0.1\linewidth}C{0.1\linewidth}|C{0.1\linewidth}}
            \hline
            $x_1$ & $x_1$ & $y$ \\ \hline
            0     & 0     & 0   \\
            0     & 1     & 1   \\
            1     & 0     & 1   \\
            1     & 1     & 1
        \end{tabular}
        \vspace{0.5cm}
        \subcaption{Truth table}
        \label{fig:perceptron:or:truth}
    \end{subfigure}\hfil
    \captionsetup{format=hang} % hanging captions
    \caption{
        \textbf{Perceptron mapping for the logical OR gate.}
    }
    \label{fig:perceptron:or}
\end{figure}

\begin{figure}[!htp]
    \centering
    \begin{subfigure}[b]{0.70\textwidth}
        \centering
        \input{graphics/tikz/perceptron_nand}
        \captionsetup{format=hang} % hanging captions
        \subcaption{Perceptron mapping}
        \label{fig:perceptron:nand:mapping}
    \end{subfigure}\hfil
    \begin{subfigure}[b]{0.29\textwidth}
        \centering
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{C{0.1\linewidth}C{0.1\linewidth}|C{0.1\linewidth}}
            \hline
            $x_1$ & $x_1$ & $y$ \\ \hline
            0     & 0     & 1   \\
            0     & 1     & 1   \\
            1     & 0     & 1   \\
            1     & 1     & 0
        \end{tabular}
        \vspace{0.5cm}
        \subcaption{Truth table}
        \label{fig:perceptron:nand:truth}
    \end{subfigure}\hfil
    \captionsetup{format=hang} % hanging captions
    \caption{
        \textbf{Perceptron mapping for the logical NAND gate.}
    }
    \label{fig:perceptron:nand}
\end{figure}

\newpage
\subsection{Basic Policy Gradient Derivation}\label{sec:pg:derivation}

Policy gradient has various forms, however, the most basic form is the deriving the gradient of the expected return of the trajectory. The derivation is as follows,

\begin{equation}
    \arraycolsep=1.4pt\def\arraystretch{2.2}
    \begin{array}{rrr}\nabla_{\theta} J\left(\pi_{\theta}\right) & =\nabla_{\theta} \underset{\tau \sim \pi_{\theta}}{\mathrm{E}}[R(\tau)] & \\ & =\nabla_{\theta} \int_{\tau} P(\tau \mid \theta) R(\tau) & \text { Expand expectation } \\ & =\int_{\tau} \nabla_{\theta} P(\tau \mid \theta) R(\tau) & \text { Bring gradient under integral } \\ & =\int_{\tau} P(\tau \mid \theta) \nabla_{\theta} \log P(\tau \mid \theta) R(\tau) & \text { Log-derivative trick } \\ & =\underset{\tau \sim \pi_{\theta}}{\mathrm{E}}\left[\nabla_{\theta} \log P(\tau \mid \theta) R(\tau)\right] & \text { Return to expectation form } \\ \therefore \nabla_{\theta} J\left(\pi_{\theta}\right) & =\underset{\tau \sim \pi_{\theta}}{\mathrm{E}}\left[\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) R(\tau)\right] & \text { Expression for grad-log-prob }\end{array}
\end{equation}
