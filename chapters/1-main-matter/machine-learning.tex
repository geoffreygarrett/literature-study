%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Machine Learning\label{chap:ML}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 2) A feedforward neural network, as formally defined in the article concerning feedforward neural networks, whose parameters are collectively denoted θ\thetaθ. In backpropagation, the parameters of primary interest are wijkw_{ij}^kwijk​, the weight between node jjj in layer lkl_klk​ and node iii in layer lk−1l_{k-1}lk−1​, and bikb_i^kbik​, the bias for node iii in layer lkl_klk​. There are no connections between nodes in the same layer and layers are fully connected.
% 3) An error function, E(X,θ)E(X, \theta)E(X,θ), which defines the error between the desired output yi⃗\vec{y_i}yi
%  and the calculated output yi⃗^\hat{\vec{y_i}}yi​​^​ of the neural network on input xi⃗\vec{x_i}xi​​ for a set of input-output pairs (xi⃗,yi⃗)∈X\big(\vec{x_i}, \vec{y_i}\big) \in X(xi​​,yi​​)∈X and a particular value of the parameters θ\thetaθ.

\Gls{ML}, often described as a proper subset of \gls{AI}
\cite{Goodfellow-et-al-2016}, and is the field in computer science which deals
with the improvement of algorithms through experience and the use of data
\cite{Mitchell97}. Our daily lives exhibit wide applications of intelligent
software to automate routine labour, understand speech or images, assist in
diagnoses in medicine and support basic scientific research. This field as a
whole is relatively young, having been coined in 1959 by Arthur Samuel
\cite{5392560}, however little progress in reaching human-comparable learning
was achieved until the advent of deep learning, termed by Rina Dechter in 1986
\cite{Rina1986}. Even then, human-like recognition of real-world images was not
achieved until ImageNet was created in 2009 \cite{5206848}, which is often
considered as the catalyst for the AI boom of the 21st century
\cite{hardy_2016}.

\begin{figure}[htp!]
    \centering
    \input{graphics/tikz/ai-ml-dl.tex}
    \captionsetup{format=hang} % hanging captions
    \caption{
        The relationship between the fields of Artificial Intelligence,
        Machine Learning, Deep Learning and Reinforcement Learning with some
        arbitrary example provided per field.
    }
    \label{fig:al-ml-dl}
\end{figure}


\newpage


\section{Machine Learning Basics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This section aims at a providing a non-exhaustive coverage of the basics of
machine learning can can be applied to all machine learning algorithms. This
section starts by defining what is meant when it is said that an algorithm
``learns". The types of datasets that may be encountered in the application of
these learning algorithms are then briefly covered to provide insight into the
potential applications which are not covered in this review. The distinction is
then made between the goal of fitting training data and the goal of finding
patterns that generalize to new data. This section then covers a very common
concept in machine learning: \textit{hyperparameters}, which are
\textit{settings} of a learning algorithm which must be determined outside the
learning algorithm itself.

\subsection{Learning algorithms}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Generally speaking, a machine learning algorithm is a procedure for learning
from data.However correct this definition is, it provides little insight into
the relevant concepts in the field. A more succint definition is provided by
Mitchell \cite{Mitchell97LearningAlgorithm}:
\begin{quotation}
    \textit{A computer program is said to learn from experience $E$ with respect
    to some class of tasks $T$ and a performance measure $P$, if its performance
    at tasks in $T$, as measured by $P$, improves with experience $E$.}
\end{quotation}
This definition introduces the general entities which are present during all
machine learning tasks. The entities will not be formally defined in the
following sections as it is far outside the scope of this literature, and is
philosophical in nature. This section will instead cover examples of each which
will provide practical insight on which the reader can build their knowledge.

\subsubsection{The Task, $T$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
There exists a plethora of tasks which humanity has applied learning algorithms
to during the timeline of the field. It is common for \gls{ML} practitioners to
originate from domains outside that of computer science, in order to access the
feasibility of existing algorithms in their domain. There is however a question
that often presents itself to specialists in their respective fields when they
consider applying \gls{ML} to the existing problems in their field. Why should a
specialist opt for solving problems with \gls{ML} that have been solved using
tried and true techniques in their domain of expertise? Goodfellow et al.
\cite{Goodfellow-et-al-2016} provide an insightful response to this:

\begin{quotation}
    \textit{Machine learning enables us to tackle tasks that are too difficult
    to solve with fixed programs written and designed by human beings. From a
    scientific and philosophical point of view, machine learning is interesting
    because developing our understanding of it entails developing our
    understanding of the principles that underlie intelligence. }
\end{quotation}
An example of a field which has undergone dramatic changes in a short period of
time, with the advent of \gls{DL} and modern hardware, is \gls{CV}. Mahony et.
al. discuss this in their paper with a focus on comparing \gls{DL} and \gls{CV}
\cite{Mahony-et-al-2020}. Their paper concludes that many \gls{CV} techniques
invented in the 20 years preceding the paper have become irrelevant as a result
of \gls{DL}. However, they emphasise on the importance of the knowledge
established in those 20 years, arguing that \textit{``knowledge is never
obsolete"}, as it provides specialists with more tools and intuition when
addressing problems. Some typical applications of \gls{CV} are detailed and
although these may be outperformed by \gls{DL}, relying on \gls{DL} in some
cases is overkill. They also point out some hybrid approaches between \gls{DL}
and \gls{CV} which synergize, saying that \gls{CV} provides improved performance
in \gls{DL} by reducing training time. This emphasises that specialists should
not expect an end-all solution from \gls{ML} in addressing their domain-specific
problems, but rather as mentioned by Goodfellow et al., strive for a better
understanding of the principles that underlie intelligence, and by extension,
those that underlie the practitioners domain-specific problems.

\begin{itemize}
    \item \textbf{Classification}: In this task, the learning algorithm is
    expected to produce a function $f: \mathbb{R^n}\rightarrow
    \{1,...,k\}$. When $y=f(\mathbf{x})$, the model assigns a provided
    input, $\mathbf{x}$ to a category identified by numeric code $y$. An
    example of this would be the mapping of a grayscale image,
    $\mathbf{x}\in\mathbb{R}^2$ to a value corresponding to a numerical
    encoding $f:\mathbb{R}^n\rightarrow\{\textrm{Cat},\;\textrm{Dog}\}$.
    \item \textbf{Classification with missing inputs}: Classification becomes
    more challenging when the input measurements to the model are not
    guaranteed to always be the same. In this situation the algorithm must
    learn the set of all function mappings arising from the possible
    combinations of input vectors that arise from missing subsets of
    inputs in $\mathbf{x}$. An example of this would be the classification
    of a diagnosis in medicine, as depending on the invasiveness of
    certain procedures, different subsets of measurements are available.
    \item \textbf{Regression}: In this task, the learning algorithm is expected
    to predict a continuous numerical value for a given input. This is
    done by learning a function $f: \mathbb{R}^n\rightarrow\mathbb{R}$.
    the formulation is similar to that of classification, except for the
    output format. An example of this would be learning of a function to
    predict the expected returns for a given investment given the state of
    the market, as is common is algorithmic trading.
    \item \textbf{Transcription}: This type of task involves the learning
    algorithm observing a relatively unstructured input and transcribing
    it into some discrete textual form. An example of this is the
    transcription of an audio waveform containing speech into text.
    \item \textbf{Machine translation}: In machine translation, the already
    structured input is mapped into a different language. This is common
    in the field of \gls{NLP}, where languages are translated between, for
    example English and French. This however is not limited to natural
    languages but can also be applied to programming languages for
    example.
    \item \textbf{Structured output}: This task entails those where the output
    is a vector or a data structure which details important relationships
    between the contained elements. This task subsumes the prior two of
    transcription and machine translation. An example of this would be the
    parsing of grammatical structure of a natural language sentence,
    addressed in \gls{NLP} and demonstrated by Collobert
    \cite{pmlr-v15-collobert11a}.
    \item \textbf{Anomaly detection}: In this task, a learning algorithm learns
    to sift through a set of events and classify events that are anomalous
    in nature. These algorithms have been applied to credit card companies
    in detection of fraud \cite{DBLP:journals/corr/abs-2108-10005}, and in
    safety in the aviation industry \cite{Janakiraman2016, Basora2019}.
    \item \textbf{Synthesis and sampling}: In this task, learning algorithms
    generate \textbf{new} examples which appear to be drawn from the same
    underlying distribution as, but are not present in the training data.
    A recent example around which a lot of research is being done is the
    \gls{GPT-3} which is able to produce human-like text, given prompts as
    input \cite{DBLP:journals/corr/abs-2005-14165}.
    \item \textbf{Imputation of missing values}: In this task, the learning
    algorithm is given some input $x\in\mathbb{R}$ with some elements
    $x_i$ missing. The algorithm must then make predictions for the
    missing values. Emmanuel et. al. discuss the latest methods in
    \gls{ML} that address this task~\cite{Emmanuel2021}.
    \item \textbf{Denoising}: In this learning task, the machine learning
    algorithm learns to generate a clean example $\mathbf{x}\in\mathbb{R}^n$
    from a corrupted sample $\tilde{\mathbf{x}}\in\mathbb{R}^n$. Generally
    speaking the learner is predicting the probability distribution
    $p(\mathbf{x}|\tilde{\mathbf{x}})$.
    \item \textbf{Density estimation} or \textbf{probability mass function estimation}
    involves the task task of learning a function $p_\text{model}:\mathbb{R}^n\rightarrow{}\mathbb{R}$,
    where $p_\text{model}(\mathbf{x})$ is interpreted as a probability distribution function
    from which $\mathbf{x}$ was drawn. Effectively the algorithm learns the
    structure of the data that it has been provided with. Most of the
    aforementioned tasks involve learning this distribution function, at least
    implicitly. For example for the missing value imputation task, if $x_i$ is
    missing, and all other values $x_{-i}$ are given, then the algorithm learns
    the distribution $p(x_i|\mathbf{x}_{-i})$ which involves $p(\mathbf{x})$
    implicitly.
\end{itemize}
%An ongoing trend in society is the augmentation of responsibility in intelligent
%algorithms through autonomy, evident by the advent of semi-autonomous cars,
%autonomous cyber-security \cite{Taeihagh2019}, autonomous industrial site
%inspection\footnote{\url{https://newsroom.ibm.com/Boston-Dynamics-and-IBM-Join-Forces-to-Bring-Mobile-Edge-Analytics-to-Industrial-Operations}},
%and autonomous supply-chain
%management\footnote{\url{https://www.forbes.com/sites/stevebanker/2021/04/01/amazon-supply-chain-innovation-continues/}},
%to name a few.

%This section reviews the current ongoing research in the field of machine
%learning as a whole. First the main categories:

\subsubsection{The Performance, $P$\label{sec:ML-performance}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In order to access the performance of our learning algorithm in task $T$,
we must have some quantitative measure of performance $P$ with which we steer
the algorithm towards the desired behaviour. $T$ and $P$ in this way are
coupled, and must $P$ must be chosen according to $T$. In this way it can be
unclear to the \gls{ML} practitioner what $P$ should be chosen.

Firstly, it is worth considering the true goal of the learning algorithm. We
wish for it to train according to some dataset such that it can generalize to
inputs that it has never seen before. After all, there is little worth in a
classification algorithm recognizing cats and dogs with 90\% accuracy on its
training dataset, if it performs with 20\% accuracy on an unseen \textbf{test}
dataset. This previously mentioned scenario is known as \textit{overfitting} and
is discussed in its own section later in the chapter. For this reason, the first
requirement on managing our dataset is presented, there must be some
\textbf{train-test split}. This is further complicated by the
\textbf{validation} dataset which is discussed later in the chapter.

Given that the input-output pair is $\{\mathbf{x}_i, \mathbf{y}_i\}$,
$\mathbf{y}_i$ is commonly referred to as the \textit{ground truth}, and
$\hat{\mathbf{f}}(\mathbf{x}_i)=\hat{\mathbf{y}}_i$ are referred to as the
\textit{predictor} and \textit{predicted output} respectively. The performance
in the form of a loss function $L$ is then some function of $\mathbf{y}_i$ and
$\hat{\mathbf{y}}_i$, therefore $L(\hat{\mathbf{y}}_i, \mathbf{y}_i)$.

\gls{ML} often borrows from \textit{approximation theory} vector norm notation
($\ell(\cdot)_p=||\cdot||_p$) to simplify notation in performance metrics. See
\autoref{appendix:vector_norms} for the expressions of vector norms.

\textbf{Regression loss functions}
\begin{itemize}
    \item \textbf{\Gls{MSE}} is the average of the squared differences between
    predictions and the ground truth. It is only concerned with the average
    magnitude of errors irrespective of their direction. Due to the squaring,
    predictions which have greater errors are more heavily penalized. A
    beneficial mathematical property of \gls{MSE} is that gradients can be
    easily calculated.
    \begin{equation}
        L_\text{MSE} = \frac{1}{N}\sum_{i=1}^{N}||\hat{\mathbf{y}}_i-\mathbf{y}_i||^2_2
    \end{equation}
    \item \textbf{L2 error} is the average of the differences between
    predictions and the ground truth. It is the same as \gls{MSE} but without
    the squaring. Like \gls{MSE}, the gradient can be easily calculated.
    \begin{equation}
        L_\text{L2E} = \frac{1}{N}\sum_{i=1}^{N}||\hat{\mathbf{y}}_i-\mathbf{y}_i||_2
    \end{equation}
    \item \textbf{\Gls{MAE}} or \textbf{L1 loss} is the average of the sum of
    absolute differences between predictions and the ground truth. This measure
    is similar to \gls{MSE} in that it considers the magnitude of the errors and
    ignores their direction. However, \gls{MAE} is more difficult to compute
    gradients for, requiring linear programming. \gls{MAE} is also more
    resilient to large error values as a result of outliers, as it does not make
    use of a square.
    \begin{equation}
        L_\text{MAE} = \frac{1}{N}\sum_{i=1}^{N}||\hat{\mathbf{y}}_i-\mathbf{y}_i||_1
    \end{equation}
    \item \textbf{\Gls{MBE}}
    \begin{equation}
        L_\text{MBE} = \frac{1}{N}\sum_{i=1}^{N}(\hat{\mathbf{y}}_i-\mathbf{y}_i)
    \end{equation}
    \item \textbf{\Gls{MSLE}}
    \begin{equation}
        \begin{aligned}
            L_\text{MSLE} &= \frac{1}{N}\sum_{i=1}^{N}(\log{(y_i+1)} - \log{(\hat{\mathbf{y}}_i+1)})^2 \\
            &= \frac{1}{N}\sum_{i=1}^{N}2\log(\frac{y_i+1}{\hat{\mathbf{y}}_i+1})
        \end{aligned}
    \end{equation}
    \item \textbf{Cosine Proximity}
    \begin{equation}
        L_\text{cos} = -\frac{\mathbf{y}_i\cdot{}\hat{\mathbf{y}}_i}{||\mathbf{y}_i||_2\cdot{}||\hat{\mathbf{y}}_i||_2}
    \end{equation}
\end{itemize}


\textbf{Binary classification loss functions}
\begin{itemize}
    \item Binary cross-entropy
    \begin{equation}
        L=-\sum_{i=1}^N\bigg(y_i\log(\hat{\mathbf{y}}_i)+(1-y_i)\log(1-\hat{\mathbf{y}}_i)\bigg)\label{eq:equation}
    \end{equation}
    \item Hinge loss
    \begin{equation}

    \end{equation}
    \item Squared hinge loss
\end{itemize}

\textbf{Multi-class classification loss functions}
\begin{itemize}
    \item Multi-class cross-entropy
    \begin{equation}
        L_{CE}=-\sum_{i=1}^N\sum_{j=1}^{M}y_{ij}\log(\hat{f}_j(\mathbf{x}_i))
    \end{equation}
    \item Hinge loss
    \item Squared hinge loss
\end{itemize}


\begin{itemize}
\end{itemize}
%With the
%In the case of classification, most metrics are derived from the confusion
%matric

\subsubsection{The Experience, $E$\label{sec:ML-experience}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Tasks provided to machine learning algorithms have different formulations based
on the experience made available. Learning algorithms can be broadly categorized
by the kind of experience that the algorithm has access to during learning, from
which it is generally expected to learn some pattern of interest.

\begin{itemize}
    \item \textbf{Supervised learning} algorithms experience a dataset
    containing features (input) and labels (expected output), and are expected
    to learn a function mapping between the two. These algorithms can be further
    divided into two categories. \textbf{Classification algorithms} learn a
    mapping from an input to a discrete class label output. \textbf{Regression
    algorithms} learn a mapping from an input to a continuous value output.

%    example input-output pairs, otherwise referred to as the labelled training
%    data consisting of a set of training examples. The trained \textit{model} is
%    then provided an unseen $x$ for which it must estimate $y$ based on its
%    experience. Supervised models are further grouped into two subcategories,
%    classification and regression. \textbf{Classification} is the task of
%    mapping $x$ to a discrete output variable $y$ which determines the predicted
%    class based on the $x$. A simple example of this would be a $y$ in
%    $\mathbb{R}^2$ which determines whether the input image was a \textit{cat}
%    or a \textit{dog}. \textbf{Regression} is task of mapping $x$ onto a
%    continuous output variable $y$ which determines the value of some variable
%    of interest. An example of this would be the task of learning an
%    approximator to the function: $f(x)=x^2$.
    % Dimensionality reduction \cite{vogelstein2021supervised}

    \item \textbf{Unsupervised learning} algorithms experience a dataset
    containing only features and learn some useful properties of the structure
    of the dataset. This form of learning often addresses recognition problems
    in \textit{association} \& \textit{clustering} \cite{barlow1999ul}.

    \item \textbf{Semi-supervised} is a middle ground between supervised
    learning (in which all training data is labelled) and unsupervised learning
    (in which no label data is provided)~\cite{books/mit/06/CSZ2006}. Some
    example applications of this paradigm are dimensionality reduction
    ~\cite{Zhang2007}, clustering \cite{Bair2013}, and anomaly detection
    \cite{DBLP:journals/corr/abs-1805-06725}.

    \item \textbf{Reinforcement learning} are learning algorithms which rely
    exclusively on a series of reinforcements from its environment. These
    reinforcements can be positive (rewards) or negative (punishments). This
    category is discussed further in \autoref{ssec:RL}.
\end{itemize}

\subsection{Capacity, Overfitting and Underfitting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The primary objective in \gls{ML} is to find a model which performs well on
previously \textit{unseen data}, which has not been used during the training or
fitting of a model. This ability of the learning algorithm is referred to as the
\textit{generalisation}.

Typically the learning algorithm is trained according to a set of
\textit{training data} with which we aim at reducing the learners
\textbf{training error}. Due to this data having been directly used to optimise
the moding during training, we cannot expect this error to be representative of
the \textbf{generalisation error}. In order to derive an estimate of the
generalisation error, we separate part of the initial data set into training and
\textbf{test data}, from which the \textbf{test error} is derived. Given some
assumptions about the \textbf{data-generating process}, it can be said that the
test error is equal to the expected generalisation error of the mode.

An important concept in \gls{ML} is the \textit{capacity} of the model. This is
defined informally as \textit{``a model’s [...] ability to fit a wide
variety of functions"}~\cite[p.~111-112]{Goodfellow-et-al-2016}.

\begin{figure}[htp]
    \centering
    \input{graphics/tikz/overfitting}
    \captionsetup{format=hang} % hanging captions
    \caption{
        Three different models fit to an arbitrary example training set
        generated from an arbitrary third-degree polynomial with some added
        noise. (a) Shows that first-degree polynomial (linear model) fails in
        generalising well to the data, and results in \textbf{underfitting}. (b)
        Shows a third-degree polynomial generalising well and is described as
        having an \textbf{appropriate capacity}, potentially optimal. (c) Shows
        an eleventh-degree polynomial which has a high variance in error and is
        described as \textbf{overfitting}.
    }
    \label{fig:mlp}
\end{figure}

\begin{figure}[htp]
    \centering
    \input{graphics/tikz/capacity-vs-error}
    \captionsetup{format=hang} % hanging captions
    \caption{
        Typical relationship between capacity and error. Training and test error
        behave differently. At the left end of the graph, training error and
        generalization error are both high. This is the \textbf{underfitting
        regime} As we increase capacity, training error decreases, but the gap
        between training and generalisation error increases. Eventually, the
        size of this gap outweighs the decrease in training error, and we enter
        the \textbf{overfitting regime}, where capacity is too large, above the
        \textbf{optimal capacity}. Adapted from Goodfellow et. al.
        \cite{Goodfellow-et-al-2016}.
    }
    \label{fig:capacity}
\end{figure}

\subsubsection{No Free Lunch Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The \gls{NFLT}, sometimes abbreviated NFL, is a simple but important concept in
\gls{ML} and optimisation. The theorem suggests that an optimisation technique
will perform equally well as any other when averaging its performance over the
set of all possible problems. This implies that there is no single best
technique for addressing an arbitrary problem. Luke states the following in
\textit{essentials of metaheuristics}~\cite{luke2012essentials}:

\begin{quotation}
    \textit{The NFL stated that within certain constraints, over the space of
    all possible problems, every optimization technique will perform as well as
    every other one on average (including Random Search)}
\end{quotation}

This argues that without having substantive information about the fundamentals
of the problem being modelled, choosing to apply a single technique to an
arbitrary problem will not yield a predictably better or worse result than
applying another to the same problem. Therefore, in the case where the
underlying process being optimised is not well-understood, a variety of
techniques should be applied.

However, in practice, knowledge to some degree is known about the problem which
is being optimised, or to which a learning algorithm is being applied. This
theorem highlights the importance of having a clear understanding of the problem
at hand before applying a learning algorithm or an optimisation technique. As
stated by Domingos~\cite{Domingos15}:

\begin{quotation}
    \textit{In the meantime, the practical consequence of the “no free lunch”
    theorem is that there’s no such thing as learning without knowledge. Data
    alone is not enough.}
\end{quotation}

This theorem, in effect, motivates the true goal of machine learning, as worded
by Goodfellow et. al.~\cite[p.~116]{Goodfellow-et-al-2016}:

\begin{quotation}
    \textit{This means that the goal of machine learning research is not to seek
    a universal learning algorithm or the absolute best learning algorithm.
    Instead, our goal is to understand what kinds of distributions are relevant
    to the “real world” that an AI agent experiences, and what kinds of machine
    learning algorithms perform well on data drawn from the kinds of
    data-generating distributions we care about.}
\end{quotation}

\subsubsection{Regularization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Regularization is any modification we make to a learning algorithm that is
intended to reduce its generalization error but not its training error

\subsection{Hyperparameters and Validation Sets}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsubsection{Resubstitution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsubsection{Holdout}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Cross-Validation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[htbp]
    \centering
    \input{graphics/tikz/kfold-cv}
    \captionsetup{format=hang} % hanging captions
    \caption{
        $k$-fold cross-validation procedure: (1) Dataset is divided into
        $k$-folds of roughly equal size. (2) Choose one fold randomly to be the
        holdout set then fit model on the remaining $k-1$ folds. (3) Iterate
        through the remaining $k-1$ folds, using each as the holdout set and
        record the error $e_i$ of the iteration. (4) Average the errors obtained
        over the $k$-folds.
    }
    \label{fig:kfold-cv}
\end{figure}

%\subsubsection{Leave-One-Out Cross-Validation (LOOCV)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsubsection{Bootstrapping}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Supervised}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Deep Learning\label{sec:DL}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Gls{DL} is a field of \gls{ML} that is primarily concerned with
the learning of representations of data. The main idea is to use a \gls{NN} to learn representations of data. The main difference between a
\gls{NN} and a traditional \gls{ML} algorithm is that the
neural network is a function of the data, rather than the data itself.

\subsection{The fundamental component: Perceptrons}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Proposed by Rosenblatt \cite{Rosenblatt_1957_6098} in his technical
report funded by the United States Office of Naval Research
\cite{doi:10.1177/030631296026003005} in 1957, the \textit{perceptron} is a
fundamental component of deep learning, describing a mathematical model of a
biological neuron. There are two different sets of notation which exist when
dealing with the bias of a perceptron. One involves the inclusion of a unit
constant in the input vector $\mathbf{x}$, with the bias being specified by the
value of the first weight $w_0$. An alternative notation exists which treats the
bias as a standalone value $b$. The latter notation will be used, as it is the
preferred notation in contemporary deep learning papers. The notation defining
the mapping of a perceptron is then: $f(\mathbf{x};\mathbf{w},
b)=\mathbf{x}^T\mathbf{w}+b$; $x\in\mathbb{R}^{d_{in}}$,
$\mathbf{w}\in\mathbb{R}^{d_{in}}$, $b\in\mathbb{R}$.

\begin{figure}[htbp]
    \centering
    \input{graphics/tikz/perceptron}
    \caption{Perceptron}
    \label{fig:perceptron}
\end{figure}

This mathematical mimicry of a biological neuron was first used used proposed by
Rosenblatt where a set of inputs $\mathbf{x}$ weighted using $\mathbf{w}$ before
being summed. If this sum surpassed a threshold, then a binary output of $1$ was
returned. \autoref{fig:perceptron} shows a more general formulation which
applies to the resulting field of deep learning, with any activation function
$\phi$ analogous for the level of excitation of a biological neuron in response
to its stimulus $\mathbf{x}$.

The primary criticism of the perceptron came in 1969 from Minsky and Papert
\cite{minsky69perceptrons}, where it was shown that the perceptron could only
solve \textit{linearly separable} functions, and failed to solve the XOR and
NXOR functions. They went on to claim that the research being done was doomed to
failure due to these limitations, resulting in little research in the area being
done until about the 1980's.

\subsection{Activation function}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Since the early days of the perceptron, a wide variety of activation functions
$\phi$ have been used and improve upon the threshold step function.

%\begin{figure}[htbp]
%    \centering
%    \input{graphics/tikz/activation}
%    \caption{Activation functions}
%    \label{fig:activation}
%\end{figure}
\input{graphics/tikz/activation}

\subsection{Multilayer perceptrons, a.k.a feed forward networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

$f(\mathbf{x};\mathbf{w}, b)=\mathbf{x}^T\mathbf{w}+b$; $x\in\mathbb{R}^{d_{in}}$, $\mathbf{w}\in\mathbb{R}^{d_{in}\times}$, $b\in\mathbb{R}^1$.

\begin{figure}
    \centering
    \input{graphics/tikz/mlp}
    \caption{Multilayer perceptron}
    \label{fig:mlp}
\end{figure}


\begin{figure}
    \centering
    \input{graphics/tikz/mlp-vec}
    \caption{Multilayer perceptron}
    \label{fig:mlp-vec}
\end{figure}


\begin{equation}
    \mathbf{h}^{(1)} = g^{(1)} \bigg(\mathbf{W}^{{(1)}\;T}\mathbf{x} + \mathbf{b}^{(1)}\bigg);
\end{equation}

\begin{equation}
    \mathbf{h}^{(2)} = g^{(2)} \bigg(\mathbf{W}^{{(2)}\;T}\mathbf{h}^{(1)} + \mathbf{b}^{(2)}\bigg);
\end{equation}

\subsection{Gradient based learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Backpropagation, short for "backward propagation of errors," is a
\textit{supervised learning} algorithm for artificial neural networks using
\textit{gradient descent}.

\begin{figure}[htp]
    \centering
    \input{graphics/tikz/gradient-descent}
    \caption{Gradient based learning}
    \label{fig:gradient-descent}
\end{figure}


\begin{figure}[htp]
    \centering
    \input{graphics/tikz/gradient-descent-3d}
    \caption{Gradient based learning}
    \label{fig:gradient-descent}
\end{figure}

\subsection{Universal Approximation Properties and Depth}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A linear model by definition, may only optimised to represent linear functions.
It has advantages in its simplicity to optimise however we often require our
estimator models to learn nonlinear functions.

%\subsection{Backpropagation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage{}


\section{Reinforcement Learning\label{ssec:RL}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Key Concepts in Reinforcement Learning}

\subsubsection{Markov Decision Processes (MDP)}

So far, we’ve discussed the agent’s environment in an informal way, but if you
try to go digging through the literature, you’re likely to run into the standard
mathematical formalism for this setting: Markov Decision Processes (MDPs). An
MDP is a 5-tuple, \langle S, A, R, P, \rho_0 \rangle, where

S is the set of all valid states,
A is the set of all valid actions,
R : $S \times A \times S \to \mathbb{R}$ is the reward function, with $r_t = R(s_t, a_t, s_{t+1})$,
P : $S \times A \to \mathcal{P}(S)$ is the transition probability function, with
$P(s'|s,a)$ being the probability of transitioning into state $s'$ if you start
in state $s$ and take action $a$,
and $\rho_0$ is the starting state distribution.

The name Markov Decision Process refers to the fact that the system obeys the
Markov property: transitions only depend on the most recent state and action,
and no prior history.

\input{graphics/tikz/agent-environment}

\begin{itemize}
    \item states and observations,
    \item action spaces,
    \item policies,
    \item trajectories,
    \item different formulations of return,
    \item the RL optimization problem,
    \item and value functions.
\end{itemize}

\subsection{Taxonomy of Reinforcement Learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Value-based methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Policy-based methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Policy gradient}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Deep deterministic policy gradient (DDPG)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
