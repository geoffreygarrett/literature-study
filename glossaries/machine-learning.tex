% MIT License
%
% Copyright (c) 2021 Geoffrey H. Garrett
%
% Permission is hereby granted, free of charge, to any person obtaining a copy
% of this software and associated documentation files (the "Software"), to deal
% in the Software without restriction, including without limitation the rights
% to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
% copies of the Software, and to permit persons to whom the Software is
% furnished to do so, subject to the following conditions:
%
% The above copyright notice and this permission notice shall be included in all
% copies or substantial portions of the Software.
%
% THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
% IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
% FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
% AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
% LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
% OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
% SOFTWARE.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ACRONYMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newacronym{NEO}{NEO}{Near-Earth object}

\newacronym{AI}{AI}{
    artificial intelligence
}
\newacronym{ML}{ML}{
    machine learning
}
\newacronym{NN}{NN}{
    neural network
}
\newacronym{ANN}{ANN}{
    artificial neural network
}
\newacronym{NLP}{NLP}{
    natural language processing
}
\newacronym{DL}{DL}{
    deep learning
}
\newacronym{RL}{RL}{
    reinforcement learning
}
\newacronym{CV}{CV}{
    computer vision
}
\newacronym[plural=MLPs, firstplural=multilayer perceptrons (CNNs)]{MLP}{MLP}{
    multilayer perceptron
}
\newacronym[plural=CNNs, firstplural=convolutional neural networks (CNNs)]{CNN}{CNN}{
    convolutional neural network
}
\newacronym[plural=RNNs, firstplural=recurrent neural networks (RNNs)]{RNN}{RNN}{
    recurrent neural network
}
\newacronym{LOOCV}{LOOCV}{
    leave-one-out cross-validation
}
\newacronym{GPT-3}{GPT-3}{
    Generative Pre-trained Transformer 3
}
\newacronym{MSE}{MSE}{
    mean squared error
}
\newacronym{MAE}{MAE}{
    mean absolute error
}
\newacronym{MBE}{MBE}{
    mean bias error
}
\newacronym{MSLE}{MSLE}{
    mean squared logarithmic error
}
\newacronym{NFLT}{NFLT}{
    No Free Lunch Theorem
}
\newacronym{NFL}{NFL}{
    No Free Lunch
}
\newacronym{CD}{CD}{
    cosine distance
}
\newacronym{CP}{CP}{
    cosine proximity
}
\newacronym{CS}{CS}{
    cosine similarity
}
\newacronym{BCE}{BCE}{
    binary cross-entropy
}
\newacronym{SVM}{SVM}{
    support vector machine
}
\newacronym{MCE}{MCE}{
    multi-class cross-entropy
}
\newacronym{HL}{HL}{
    hinge loss
}
\newacronym{MHL}{MHL}{
    multi-class hinge loss
}
\newacronym{SHL}{SHL}{
    squared hinge loss
}
\newacronym{MSHL}{MSHL}{
    multi-class squared hinge loss
}

\newacronym{LASSO}{LASSO}{
    least absolute shrinkage and selection operator
}
\newacronym{RIFE}{RIFE}{
    real-time intermediate flow estimation for video frame interpolation
}
\newacronym{KFCV}{KFCV}{
    $k$-fold cross-validation
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NOTATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newglossary{notation:ml}{nml}{sbl}{Machine Learning}

\newglossaryentry{b}{
    type=notation:ml,
    name=\ensuremath{b},
    sort=b,
    description={bias}
}
\newglossaryentry{ml:x}{
    type=notation:ml,
    name=\ensuremath{\bm{x}},
    sort=x,
    description={input vector}
}
\newglossaryentry{ml:n_x}{
    type=notation:ml,
    name=\ensuremath{n_x},
    sort=n_x,
    description={input size}
}
\newglossaryentry{ml:m}{
    type=notation:ml,
    name=\ensuremath{m},
    sort=m,
    description={number for examples in the dataset}
}
\newglossaryentry{ml:n_y}{
    type=notation:ml,
    name=\ensuremath{n_y},
    sort=n_y,
    description={output size}
}
\newglossaryentry{ml:x_i}{
    type=notation:ml,
    name=\ensuremath{\bm{x}^{(i)}},
    sort=x_i,
    description={input vector for the $i^\text{th}$ example}
}
\newglossaryentry{y_true}{
    type=notation:ml,
    name=\ensuremath{\bm{y}},
    sort=y_true,
    description={ground truth/ output vector}
}
\newglossaryentry{y_true_i}{
    type=notation:ml,
    name=\ensuremath{\bm{y}^{(i)}},
    sort=y_true_i,
    description={ground truth/ output vector for the $i^\text{th}$ example}
}
\newglossaryentry{y_true_ij}{
    type=notation:ml,
    name=\ensuremath{{y}^{(i)}_j},
    sort=y_true_ij,
    description={ground truth/ output vector for the $j^\text{th}$ component of the $i^\text{th}$ example}
}
\newglossaryentry{y_true_i1}{
    type=notation:ml,
    name=\ensuremath{{y}^{(i)}},
    sort=y_true_i1,
    description={ground truth/ output value for the $i^\text{th}$ example}
}
\newglossaryentry{w_vec}{
    type=notation:ml,
    name=\ensuremath{\bm{w}},
    sort=w_vec,
    description={weight vector}}
\newglossaryentry{Y}{
    type=notation:ml,
    name=\ensuremath{\bm{Y}},
    sort=Y,
    description={ground truth/ output matrix of all \gls{ml:m} examples in the dataset}
}
\newglossaryentry{X}{
    type=notation:ml,
    name=\ensuremath{\bm{X}},
    sort=X,
    description={input matrix of all  \gls{ml:m} examples in the dataset}
}

\newglossaryentry{ml:y_true:i:i+m}{
    type=notation:ml,
    name=\ensuremath{\bm{y}^{(i:i+m)}},
    sort=y_pred_i_i_m,
    description={matrix of true output vectors for the $i^\text{th}$ example to the $i+m^\text{th}$ example}
}

\newglossaryentry{ml:y_true:i}{
    type=notation:ml,
    name=\ensuremath{\bm{y}^{(i)}},
    sort=y_pred_i,
    description={ground truth output vector for the $i^\text{th}$ example}
}

\newglossaryentry{ml:x:i}{
    type=notation:ml,
    name=\ensuremath{\bm{x}^{(i)}},
    sort=x_i,
    description={input vector for the $i^\text{th}$ example}
}

\newglossaryentry{ml:x:i:i+m}{
    type=notation:ml,
    name=\ensuremath{\bm{x}^{(i:i+m)}},
    sort=x_i_i_m,
    description={matrix of input vectors for the $i^\text{th}$ example to the $i+m^\text{th}$ example}
}

\newglossaryentry{y_pred}{
    type=notation:ml,
    name=\ensuremath{\hat{\bm{y}}},
    sort=x_pred,
    description={predicted output vector}
}
\newglossaryentry{y_pred_i}{
    type=notation:ml,
    name=\ensuremath{\hat{\bm{y}}^{(i)}},
    sort=y_pred_i,
    description={predicted output vector for the $i^\text{th}$ example}
}
\newglossaryentry{y_pred_ij}{
    type=notation:ml,
    name=\ensuremath{\hat{{y}}^{(i)}_j},
    sort=y_pred_ij,
    description={predicted output vector for the $j^\text{th}$ component of the $i^\text{th}$ example}
}
\newglossaryentry{y_pred_i1}{
    type=notation:ml,
    name=\ensuremath{\hat{{y}}^{(i)}},
    sort=y_pred_i1,
    description={predicted output value for the $i^\text{th}$ example}
}
\newglossaryentry{ml:theta}{
    type=notation:ml,
    name=\ensuremath{{\bm{\theta}}},
    sort=theta,
    description={model parameters}
}
\newglossaryentry{o_reg}{
    type=notation:ml,
    name=\ensuremath{{{\Omega}}},
    sort=Omega,
    description={regularization function}
}
\newglossaryentry{J}{
    type=notation:ml,
    name=\ensuremath{{{J}}},
    sort=J,
    description={objective function}
}

\newglossaryentry{ml:J}{
    type=notation:ml,
    name=\ensuremath{{{J}}},
    sort=J,
    description={objective function}
}
\newglossaryentry{J_reg}{
    type=notation:ml,
    name=\ensuremath{{{\tilde{J}}}},
    sort=Jtilde,
    description={regularised objective function}
}
\newglossaryentry{ml:w_reg}{
    type=notation:ml,
    name=\ensuremath{{\alpha}},
    sort=alpha,
    description={norm penalty weight}
}
\newglossaryentry{b_ela}{
    type=notation:ml,
    name=\ensuremath{{\beta}},
    sort=beta,
    description={mixing parameter between $L^2$ ($\beta=1$) and $L^1$ ($\beta=0$) regularization}
}

% deep learning

\newacronym{MLPs}{MLPs}{multi-layer perceptrons}
\newacronym{ANN}{ANN}{artificial neural networks}
\newacronym{GD}{GD}{gradient descent}
\newacronym{SGD}{SGD}{stochastic gradient descent}
\newacronym{NAG}{NAG}{Nesterov accelerated gradient}
\newacronym{MB-SGD}{MB-SGD}{mini-batch stochastic gradient descent}
\newacronym{adagrad}{AdaGrad}{adaptive gradient}
\newacronym{adam}{Adam}{adaptive moment estimation}
\newglossaryentry{theta}{type=symbols,name=\ensuremath{\mathbf{\theta}},sort=theta,description={model parameters}}
\newglossaryentry{L}{type=symbols,name=\ensuremath{L},sort=L,description={number of hidden layers in the network}}
\newglossaryentry{a_vec}{type=symbols,name=\ensuremath{\mathbf{a}},sort=a,description={hidden layer output}}

\newglossary{notation:dl}{ndl}{sbl}{Deep Learning}

\newglossaryentry{dl:L}{
    type=notation:dl,
    name=\ensuremath{L},
    sort=L,
    description={number of hidden layers in the network}
}

\newglossaryentry{dl:activation}{
    type=notation:dl,
    name=\ensuremath{\phi},
    sort=phi,
    description={activation function}
}

\newglossaryentry{dl:J}{
    type=notation:dl,
    name=\ensuremath{{{J}}},
    sort=J,
    description={objective function}
}

\newglossaryentry{dl:theta}{
    type=notation:dl,
    name=\ensuremath{\bm{\theta}},
    sort=theta,
    description={vector of model parameters}
}

\newglossaryentry{dl:theta:j}{
    type=notation:dl,
    name=\ensuremath{\theta_j},
    sort=theta:j,
    description={$j^{\text{th}}$ model parameter}
}

\newglossaryentry{dl:nh:l}{
    type=notation:dl,
    name=\ensuremath{n_h^{[l]}},
    sort=nhl,
    description={number of hidden units in the $\gls{dl:l}^\text{th}$ layer of the network}
}
\newglossaryentry{dl:nh:lm1}{
    type=notation:dl,
    name=\ensuremath{n_h^{[l-1]}},
    sort=nhlm1,
    description={number of hidden units in the $(\gls{dl:l}-1)^\text{th}$ layer of the network}
}

\newglossaryentry{dl:nh:lm1}{
    type=notation:dl,
    name=\ensuremath{n_h^{[\gls{dl:l}-1]}},
    sort=nhlm1,
    description={number of hidden units in the ${(\gls{dl:l}-1})^\text{th}$ layer of the network}
}

\newglossaryentry{dl:nh:L}{
    type=notation:dl,
    name=\ensuremath{n_h^{[\gls{dl:L}]}},
    sort=nhL,
    description={number of hidden units in the ${\gls{dl:L}}^\text{th}$ layer of the network}
}

\newglossaryentry{dl:nh:Lm1}{
    type=notation:dl,
    name=\ensuremath{n_h^{[\gls{dl:L}-1]}},
    sort=nhLm1,
    description={number of hidden units in the ${(\gls{dl:L}-1)}^\text{th}$ layer of the network}
}

\newglossaryentry{dl:nh:1}{
    type=notation:dl,
    name=\ensuremath{n_h^{[1]}},
    sort=nh1,
    description={number of hidden units in the 1$^\text{st}$ layer of the network}
}

\newglossaryentry{dl:nh:2}{
    type=notation:dl,
    name=\ensuremath{n_h^{[2]}},
    sort=nh2,
    description={number of hidden units in the 2$^\text{nd}$ layer of the network}
}

\newglossaryentry{dl:W:l}{
    type=notation:dl,
    name=\ensuremath{\bm{W}^{[l]}},
    sort=Wl,
    description={$\in\gls{set:R}^{\gls{dl:nh:l}\times\gls{dl:nh:lm1}}$, weight matrix of the \gls{dl:l}$^\text{th}$ layer of the network}
}

\newglossaryentry{dl:lr}{
    type=notation:dl,
    name=\ensuremath{\eta},
    sort=\alpha,
    description={learning rate, a.k.a. \textit{step-size}, of the network update}
}

\newglossaryentry{dl:J}{
    type=notation:dl,
    name=\ensuremath{\alpha},
    sort=\alpha,
    description={learning rate, a.k.a. \textit{step-size}, of the network update}
}


\newglossaryentry{dl:a:l:j}{
    type=notation:dl,
    name=\ensuremath{a^{[l]}_j},
    sort=al,
    description={}
}
\newglossaryentry{dl:a:lm1:k}{
    type=notation:dl,
    name=\ensuremath{a^{[l-1]}_k},
    sort=alm1,
    description={}
}
\newglossaryentry{dl:a:l}{
    type=notation:dl,
    name=\ensuremath{\bm{a}^{[\gls{dl:l}]}},
    sort=al,
    description={}
}

\newglossaryentry{dl:W:L}{
    type=notation:dl,
    name=\ensuremath{\bm{W}^{[\gls{dl:L}]}},
    sort=WL,
    description={$\in\gls{set:R}^{\gls{dl:nh:L}\times\gls{dl:nh:Lm1}}$, weight matrix of the \gls{dl:L} (output) layer of the network}
}

%\newglossaryentry{dl:W:0}{
%    type=notation:dl,
%    name=\ensuremath{\bm{W}^{[0]}},
%    sort=W0,
%    description={weight matrix of the 0$^\text{th}$ (input) layer of the network}
%}

\newglossaryentry{dl:W:1}{
    type=notation:dl,
    name=\ensuremath{\bm{W}^{[1]}},
    sort=W1,
    description={$\in\gls{set:R}^{\gls{dl:nh:1}\times\gls{ml:n_x}}$, weight matrix of the 1$^\text{st}$ (input) layer of the network}
}

\newglossaryentry{dl:W:2}{
    type=notation:dl,
    name=\ensuremath{\bm{W}^{[2]}},
    sort=W2,
    description={$\in\gls{set:R}^{\gls{dl:nh:2}\times\gls{dl:nh:1}}$, weight matrix of the 2$^\text{nd}$ layer of the network}
}

\newglossaryentry{dl:W:2}{
    type=notation:dl,
    name=\ensuremath{\bm{W}^{[2]}},
    sort=W2,
    description={weight matrix of the 2$^\text{nd}$ layer of the network}
}

\newglossaryentry{dl:z}{
    type=notation:dl,
    name=\ensuremath{z},
    sort=z,
    description={latent variable}
}

\newglossaryentry{dl:a}{
    type=notation:dl,
    name=\ensuremath{a},
    sort=a,
    description={activation variable}
}
\newglossaryentry{dl:l}{
    type=notation:dl,
    name=\ensuremath{l},
    sort=l,
    description={l$^{th}$ layer of the network}
}
\newglossaryentry{dl:w:l:jk}{
    type=notation:dl,
    name=\ensuremath{{w}^{[\gls{dl:l}]}_{jk}},
    sort=wljk,
    description={}
}
\newglossaryentry{dl:w:l}{
    type=notation:dl,
    name=\ensuremath{{\bm{W}}^{[l]}},
    sort=Wl,
    description={}
}
\newglossaryentry{dl:w:1:22}{
    type=notation:dl,
    name=\ensuremath{{w}^{[1]}_{22}},
    sort=w122,
    description={}
}
\newglossaryentry{dl:w:2:11}{
    type=notation:dl,
    name=\ensuremath{{w}^{[2]}_{11}},
    sort=a,
    description={}
}
\newglossaryentry{dl:w:2:21}{
    type=notation:dl,
    name=\ensuremath{{w}^{[2]}_{21}},
    sort=a,
    description={}
}
\newglossaryentry{dl:w:1:11}{
    type=notation:dl,
    name=\ensuremath{{w}^{[1]}_{11}},
    sort=a,
    description={}
}
\newglossaryentry{dl:w:1:12}{
    type=notation:dl,
    name=\ensuremath{{w}^{[1]}_{12}},
    sort=a,
    description={}
}
\newglossaryentry{dl:w:1:21}{
    type=notation:dl,
    name=\ensuremath{{w}^{[1]}_{21}},
    sort=a,
    description={}
}
\newglossaryentry{dl:w:2:12}{
    type=notation:dl,
    name=\ensuremath{{w}^{[2]}_{12}},
    sort=a,
    description={}
}
\newglossaryentry{dl:w:2:22}{
    type=notation:dl,
    name=\ensuremath{{w}^{[2]}_{22}},
    sort=a,
    description={}
}
\newglossaryentry{dl:w:2:1}{
    type=notation:dl,
    name=\ensuremath{{w}^{[1]}_2},
    sort=a,
    description={weight of 2$^\text{nd}$ node of the 0$^\text{th}$ layer of the network (equivalently $x_1$)}
}

\newglossaryentry{dl:z:2:1}{
    type=notation:dl,
    name=\ensuremath{{z}^{[2]}_1},
    sort=a,
    description={}
}

\newglossaryentry{dl:z:vec:l}{
    type=notation:dl,
    name=\ensuremath{\bm{z}^{[l]}},
    sort=a,
    description={}
}

\newglossaryentry{dl:z:1:1}{
    type=notation:dl,
    name=\ensuremath{{z}^{[1]}_1},
    sort=z11,
    description={latent variable of 1$^\text{st}$ node of the 1$^\text{st}$ layer of the network}
}

\newglossaryentry{dl:z:1:2}{
    type=notation:dl,
    name=\ensuremath{{z}^{[1]}_2},
    sort=z12,
    description={latent variable of 2$^\text{nd}$ node of the 1$^\text{st}$ layer of the network}
}

\newglossaryentry{dl:z:l:j}{
    type=notation:dl,
    name=\ensuremath{{z}^{[\gls{dl:l}]}_j},
    sort=a,
    description={weight of j$^\text{th}$ node of the \gls{dl:l}$^\text{th}$ layer of the network}
}

\newglossaryentry{dl:a:0:1}{
    type=notation:dl,
    name=\ensuremath{{a}^{[0]}_1},
    sort=a,
    description={activation output of 1$^\text{st}$ node of the 0$^\text{th}$ layer of the network (equivalently $x_1$)}
}

\newglossaryentry{dl:a:0:2}{
    type=notation:dl,
    name=\ensuremath{{a}^{[0]}_2},
    sort=a,
    description={activation output of 2$^\text{nd}$ node of the 0$^\text{th}$ layer of the network (equivalently $x_2$)}
}

\newglossaryentry{dl:a:1:1}{
    type=notation:dl,
    name=\ensuremath{{a}^{[1]}_1},
    sort=a,
    description={activation output of 1$^\text{st}$ node of the 1$^\text{st}$ layer of the network}
}

\newglossaryentry{dl:a:1:2}{
    type=notation:dl,
    name=\ensuremath{{a}^{[1]}_2},
    sort=a,
    description={activation output of 2$^\text{nd}$ node of the 1$^\text{st}$ layer of the network}
}

\newglossaryentry{dl:a:2:1}{
    type=notation:dl,
    name=\ensuremath{{a}^{[2]}_1},
    sort=a,
    description={activation output of 1$^\text{st}$ node of the 2$^\text{nd}$ layer of the network}
}

\newglossaryentry{dl:a:2:2}{
    type=notation:dl,
    name=\ensuremath{{a}^{[2]}_2},
    sort=a,
    description={}
}

\newglossaryentry{dl:z:2:2}{
    type=notation:dl,
    name=\ensuremath{{z}^{[2]}_2},
    sort=z,
    description={}
}

\newglossaryentry{dl:a:vec:l}{
    type=notation:dl,
    name=\ensuremath{\bm{a}^{[l]}},
    sort=a1,
    description={activation output vector of the $l^\text{th}$ layer of the network}
}

\newglossaryentry{dl:a:vec:0}{
    type=notation:dl,
    name=\ensuremath{\bm{a}^{[0]}},
    sort=a1,
    description={}
}

\newglossaryentry{dl:a:vec:lm1}{
    type=notation:dl,
    name=\ensuremath{\bm{a}^{[l-1]}},
    sort=am1,
    description={activation output vector of the $(l-1)^\text{th}$ layer of the network}
}

\newglossaryentry{dl:a:vec:1}{
    type=notation:dl,
    name=\ensuremath{\bm{a}^{[1]}},
    sort=a1,
    description={activation output vector of the $1^\text{st}$ layer of the network}
}

\newglossaryentry{dl:a:vec:2}{
    type=notation:dl,
    name=\ensuremath{\bm{a}^{[2]}},
    sort=a2,
    description={activation output vector of the $2^\text{nd}$ layer of the network}
}

\newglossaryentry{dl:a:vec:l}{
    type=notation:dl,
    name=\ensuremath{\bm{a}^{[\gls{dl:l}]}},
    sort=al,
    description={activation output vector of the $\gls{dl:l}^\text{th}$ layer of the network}
}

\newglossaryentry{dl:a:vec:L}{
    type=notation:dl,
    name=\ensuremath{\bm{a}^{[\gls{dl:L}]}},
    sort=aL,
    description={activation output vector of the $\gls{dl:L}^\text{th}$ layer of the network}
}

\newglossaryentry{dl:a:vec:Lm1}{
    type=notation:dl,
    name=\ensuremath{\bm{a}^{[\gls{dl:L}-1]}},
    sort=aLm1,
    description={activation output vector of the $(\gls{dl:L}-1)^\text{th}$ layer of the network}
}

\newglossaryentry{dl:b:l:j}{
    type=notation:dl,
    name=\ensuremath{b^{[\gls{dl:l}]}_j},
    sort=blj,
    description={bias value of the $j^\text{th}$ node of the $\gls{dl:l}^\text{th}$ layer of the network}
}

\newglossaryentry{dl:b:l}{
    type=notation:dl,
    name=\ensuremath{\bm{b}^{[l]}},
    sort=bl,
    description={bias parameter vector of the $l^\text{th}$ layer of the network}
}

\newglossaryentry{dl:b:1:1}{
    type=notation:dl,
    name=\ensuremath{b^{[1]}_1},
    sort=b11,
    description={}
}
\newglossaryentry{dl:b:1:2}{
    type=notation:dl,
    name=\ensuremath{b^{[1]}_2},
    sort=b12,
    description={}
}
\newglossaryentry{dl:b:2:1}{
    type=notation:dl,
    name=\ensuremath{b^{[2]}_1},
    sort=b21,
    description={}
}
\newglossaryentry{dl:b:2:2}{
    type=notation:dl,
    name=\ensuremath{b^{[2]}_2},
    sort=b22,
    description={}
}
\newglossaryentry{dl:b:vec:1}{
    type=notation:dl,
    name=\ensuremath{\bm{b}^{[1]}},
    sort=b1,
    description={bias vector of the $1^\text{st}$ layer of the network}
}

\newglossaryentry{dl:b:vec:2}{
    type=notation:dl,
    name=\ensuremath{\bm{b}^{[2]}},
    sort=b2,
    description={bias vector of the $2^\text{nd}$ layer of the network}
}

\newglossaryentry{dl:b:vec:l}{
    type=notation:dl,
    name=\ensuremath{\bm{b}^{[\gls{dl:l}]}},
    sort=bl,
    description={bias vector of the $\gls{dl:l}^\text{th}$ layer of the network}
}

\newglossaryentry{dl:g:1}{
    type=notation:dl,
    name=\ensuremath{g^{[1]}},
    sort=g1,
    description={activation function of the $1^\text{st}$ layer of the network}
}

\newglossaryentry{dl:g:2}{
    type=notation:dl,
    name=\ensuremath{g^{[2]}},
    sort=g2,
    description={activation function of the $2^\text{nd}$ layer of the network}
}

\newglossaryentry{dl:g:l}{
    type=notation:dl,
    name=\ensuremath{g^{[\gls{dl:l}]}},
    sort=gl,
    description={activation function of the $\gls{dl:l}^\text{th}$ layer of the network}
}

\newglossaryentry{dl:b:vec:l}{
    type=notation:dl,
    name=\ensuremath{\bm{b}^{[\gls{dl:l}]}},
    sort=bl,
    description={bias vector of the $\gls{dl:l}^\text{th}$ layer of the network}
}

\newglossaryentry{dl:theta:j}{
    type=notation:dl,
    name=\ensuremath{\theta_j},
    sort=i,   % is this
    description={model parameters at state $j$}
}


%\newglossaryentry{dl:a:1}{
%    type=notation:dl,
%    name=\ensuremath{\bm{W}^{[2]}},
%    sort=W2,
%    description={weight matrix of the 2$^\text{nd}$ layer of the network}
%}
%
%\newglossaryentry{dl:a:2}{
%    type=notation:dl,
%    name=\ensuremath{\bm{W}^{[2]}},
%    sort=W2,
%    description={weight matrix of the 2$^\text{nd}$ layer of the network}
%}


% reinforcement learning
\newglossaryentry{value}{type=symbols, name=\ensuremath{V},sort=V, description={value function}}
\newglossaryentry{Q}{type=symbols, name=\ensuremath{Q},sort=Q, description={action-value function}}
\newglossaryentry{state}{type=symbols, name=\ensuremath{s},sort=s, description={state}}
\newglossaryentry{action}{type=symbols, name=\ensuremath{a},sort=a, description={action}}
\newglossaryentry{action0}{type=symbols, name=\ensuremath{a_0},sort=a_0, description={initial action}}
\newglossaryentry{state0}{type=symbols, name=\ensuremath{s_0},sort=s_0, description={initial state}}
\newglossaryentry{trajectory}{type=symbols, name=\ensuremath{\tau},sort=tau, description={the trajectory, i.e. the sequence of states and actions in the world}}
\newglossaryentry{policy}{type=symbols, name=\ensuremath{\pi},sort=pi, description={the policy, i.e. a rule used by an agent to decide what action to take}}
