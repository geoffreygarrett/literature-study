%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Machine Learning\label{chap:ML}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 2) A feedforward neural network, as formally defined in the article concerning feedforward neural networks, whose parameters are collectively denoted θ\thetaθ. In backpropagation, the parameters of primary interest are wijkw_{ij}^kwijk​, the weight between node jjj in layer lkl_klk​ and node iii in layer lk−1l_{k-1}lk−1​, and bikb_i^kbik​, the bias for node iii in layer lkl_klk​. There are no connections between nodes in the same layer and layers are fully connected.
% 3) An error function, E(X,θ)E(X, \theta)E(X,θ), which defines the error between the desired output yi⃗\vec{y_i}yi
%  and the calculated output yi⃗^\hat{\vec{y_i}}yi​​^​ of the neural network on input xi⃗\vec{x_i}xi​​ for a set of input-output pairs (xi⃗,yi⃗)∈X\big(\vec{x_i}, \vec{y_i}\big) \in X(xi​​,yi​​)∈X and a particular value of the parameters θ\thetaθ.

\Gls{ML}, often described as a proper subset of \gls{AI}
\cite{Goodfellow-et-al-2016}, and is the field in computer science which deals
with the improvement of algorithms through experience and the use of data
\cite{Mitchell97}. Our daily lives exhibit wide applications of intelligent
software to automate routine labour, understand speech or images, assist in
diagnoses in medicine and support basic scientific research. This field as a
whole is relatively young, having been coined in 1959 by Arthur Samuel
\cite{5392560}, however little progress in reaching human-comparable learning
was achieved until the advent of deep learning, termed by Rina Dechter in 1986
\cite{Rina1986}. Even then, human-like recognition of real-world images was not
achieved until ImageNet was created in 2009 \cite{5206848}, which is often
considered as the catalyst for the AI boom of the 21st century
\cite{hardy_2016}.

\begin{figure}[htp!]
    \centering
    \input{graphics/tikz/ai-ml-dl.tex}
    \caption{
        The relationship between the fields of Artificial Intelligence,
        Machine Learning, Deep Learning and Reinforcement Learning.
    }
    \label{fig:al-ml-dl}
\end{figure}


\newpage


\section{Machine Learning Basics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This section aims at a providing a non-exhaustive coverage of the basics of
machine learning can can be applied to all machine learning algorithms. This
section starts by defining what is meant when it is said that an algorithm
``learns". The types of datasets that may be encountered in the application of
these learning algorithms are then briefly covered to provide insight into the
potential applications which are not covered in this review. The distinction is
then made between the goal of fitting training data and the goal of finding
patterns that generalize to new data. This section then covers a very common
concept in machine learning: \textit{hyperparameters}, which are
\textit{settings} of a learning algorithm which must be determined outside the
learning algorithm itself.

\subsection{Learning algorithms}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Generally speaking, a machine learning algorithm is a procedure for learning
from data.However correct this definition is, it provides little insight into
the relevant concepts in the field. A more succint definition is provided by
Mitchell \cite{Mitchell97LearningAlgorithm}:
\begin{quotation}
    \textit{
        A computer program is said to learn from experience $E$ with respect to
        some class of tasks $T$ and a performance measure $P$, if its
        performance at tasks in $T$, as measured by $P$, improves with
        experience $E$.
    }
\end{quotation}
This definition introduces the general entities which are present during all
machine learning tasks. The entities will not be formally defined in the
following sections as it is far outside the scope of this literature, and is
philosophical in nature. This section will instead cover examples of each which
will provide practical insight on which the reader can build their knowledge.

\subsubsection{The Task, $T$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
There exists a plethora of tasks which humanity has applied learning algorithms
to during the timeline of the field. It is common for \gls{ML} practitioners to
originate from domains outside that of computer science, in order to access the
feasibility of existing algorithms in their domain. There is however a question
that often presents itself to specialists in their respective fields when they
consider applying \gls{ML} to the existing problems in their field. Why should a
specialist opt for solving problems with \gls{ML} that have been solved using tried
and true techniques in their domain of expertise? Goodfellow et al.
\cite{Goodfellow-et-al-2016} provide an insightful response to this:

\begin{quotation}
    \textit{
        Machine learning enables us to tackle tasks that are too difficult to
        solve with fixed programs written and designed by human beings. From a
        scientific and philosophical point of view, machine learning is
        interesting because developing our understanding of it entails
        developing our understanding of the principles that underlie
        intelligence.
    }
\end{quotation}
An example of a field which has undergone dramatic changes in a short period of
time, with the advent of \gls{DL} and modern hardware, is \gls{CV}. Mahony et.
al. discuss this in their paper with a focus on comparing \gls{DL} and \gls{CV}
\cite{Mahony-et-al-2020}. Their paper concludes that many \gls{CV} techniques
invented in the 20 years preceding the paper have become irrelevant as a result
of \gls{DL}. However, they emphasise on the importance of the knowledge
established in those 20 years, arguing that \textit{``knowledge is never
obsolete"}, as it provides specialists with more tools and intuition when
addressing problems. Some typical applications of \gls{CV} are detailed and
although these may be outperformed by \gls{DL}, relying on \gls{DL} in some
cases is overkill. They also point out some hybrid approaches between \gls{DL}
and \gls{CV} which synergize, saying that \gls{CV} provides improved performance
in \gls{DL} by reducing training time. This emphasizes that specialists should
not expect an end-all solution from \gls{ML} in addressing their domain-specific
problems, but rather as mentioned by Goodfellow et al., strive for a better
understanding of the principles that underlie intelligence, and by extension,
those that underlie the practitioners domain-specific problems.

\begin{itemize}
    \item \textbf{Classification}: In this task, the learning algorithm is
          expected to produce a function $f: \mathbb{R^n}\rightarrow
          \{1,...,k\}$. When $y=f(\mathbf{x})$, the model assigns a provided
          input, $\mathbf{x}$ to a category identified by numeric code $y$. An
          example of this would be the mapping of a grayscale image,
          $\mathbf{x}\in\mathbb{R}^2$ to a value corresponding to a numerical
          encoding $f:\mathbb{R}^n\rightarrow\{\textrm{Cat},\;\textrm{Dog}\}$.
    \item \textbf{Classification with missing inputs}: Classification becomes
          more challenging when the input measurements to the model are not
          guaranteed to always be the same. In this situation the algorithm must
          learn the set of all function mappings arising from the possible
          combinations of input vectors that arise from missing subsets of
          inputs in $\mathbf{x}$. An example of this would be the classification
          of a diagnosis in medicine, as depending on the invasiveness of of
          certain procedures, different subsets of measurements are available.
    \item \textbf{Regression}: In this task, the learning algorithm is expected
          to predict a continuous numerical value for a given input. This is
          done by learning a function $f: \mathbb{R^n}\rightarrow\mathbb{R}$.
          the formulation is similar to that of classification, except for the
          output format. An example of this would be learning of a function to
          predict the expected returns for a given investment given the state of
          the market, as is common is algorithmic trading.
    \item \textbf{Transcription}: This type of task involves the learning
          algorithm observing a relatively unstructured input and transcribing
          it into some discrete textual form. An example of this is the
          transcription of an audio waveform containing speech into text.
    \item \textbf{Machine translation}: In machine translation, the already
          structured input is mapped into a different language. This is common
          in the field of \gls{NLP}, where languages are translated between, for
          example English and French. This however is not limited to natural
          languages but can also be applied to programming languages for
          example.
    \item \textbf{Structured output}
    \item \textbf{Anomaly detection}
    \item \textbf{Synthesis and sampling}
    \item \textbf{Imputation of missing values}
    \item \textbf{Denoising}
    \item \textbf{Density estimation} or \textbf{probability mass function estimation}
\end{itemize}
%An ongoing trend in society is the augmentation of responsibility in intelligent
%algorithms through autonomy, evident by the advent of semi-autonomous cars,
%autonomous cyber-security \cite{Taeihagh2019}, autonomous industrial site
%inspection\footnote{\url{https://newsroom.ibm.com/Boston-Dynamics-and-IBM-Join-Forces-to-Bring-Mobile-Edge-Analytics-to-Industrial-Operations}},
%and autonomous supply-chain
%management\footnote{\url{https://www.forbes.com/sites/stevebanker/2021/04/01/amazon-supply-chain-innovation-continues/}},
%to name a few.

%This section reviews the current ongoing research in the field of machine
%learning as a whole. First the main categories:

\subsubsection{The Performance, $P$\label{sec:ML-performance}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In order to access the performance of our learning algorithm in task $T$,
we must have some quantitative measure of performance $P$ with which we steer
the algorithm towards the desired behaviour. $T$ and $P$ in this way are
coupled, and must $P$ must be chosen according to $T$. In this way it can be
unclear to the \gls{ML} practitioner what $P$ should be chosen.

Firstly, it is worth considering the true goal of the learning algorithm. We
wish for it to train according to some dataset such that it can generalize to
inputs that it has never seen before. After all, there is little worth in a
classification algorithm recognizing cats and dogs with 90\% accuracy on its
training dataset, if it performs with 20\% accuracy on an unseen \textbf{test}
dataset. This previously mentioned scenario is known as \textit{overfitting} and
is discussed in its own section later in the chapter. For this reason, the first
requirement on managing our dataset is presented, there must be some
\textbf{train-test split}. This is further complicated by the
\textbf{validation} dataset which is discussed later in the chapter.

With the
In the case of classification, most metrics are derived from the confusion
matric


\subsubsection{The Experience, $E$\label{sec:ML-experience}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Tasks provided to machine learning algorithms have different formulations based
on the experience made available. Learning algorithms can be broadly categorized
by the kind of experience that the algorithm has access to during learning, from
which it is generally expected to learn some pattern of interest.

\begin{itemize}
    \item \textbf{Supervised learning} algorithms experience a dataset
    containing features (input) and labels (expected output), and are expected
    to learn a function mapping between the two. These algorithms can be further
    divided into two categories. \textbf{Classification algorithms} learn a
    mapping from an input to a discrete class label output. \textbf{Regression
    algorithms} learn a mapping from an input to a continuous value output.

%    example input-output pairs, otherwise referred to as the labelled training
%    data consisting of a set of training examples. The trained \textit{model} is
%    then provided an unseen $x$ for which it must estimate $y$ based on its
%    experience. Supervised models are further grouped into two subcategories,
%    classification and regression. \textbf{Classification} is the task of
%    mapping $x$ to a discrete output variable $y$ which determines the predicted
%    class based on the $x$. A simple example of this would be a $y$ in
%    $\mathbb{R}^2$ which determines whether the input image was a \textit{cat}
%    or a \textit{dog}. \textbf{Regression} is task of mapping $x$ onto a
%    continuous output variable $y$ which determines the value of some variable
%    of interest. An example of this would be the task of learning an
%    approximator to the function: $f(x)=x^2$.
    % Dimensionality reduction \cite{vogelstein2021supervised}

    \item \textbf{Unsupervised learning} algorithms experience a dataset
    containing only features and learn some useful properties of the structure
    of the dataset. This form of learning often addresses recognition problems
    in \textit{association} \& \textit{clustering} \cite{barlow1999ul}.

    \item \textbf{Semi-supervised} is a middle ground between supervised
    learning (in which all training data is labelled) and unsupervised learning
    (in which no label data is provided)~\cite{books/mit/06/CSZ2006}. Some
    example applications of this paradigm are dimensionality reduction
    ~\cite{Zhang2007}, clustering \cite{Bair2013}, and anomaly detection
    \cite{DBLP:journals/corr/abs-1805-06725}.

    \item \textbf{Reinforcement learning} is a learning task which relies
    exclusively on a series of reinforcements. These reinforcements can be
    positive (rewards) or negative (punishments). This category is discussed
    further in \autoref{ssec:RL}.
\end{itemize}

\subsection{Capacity, Overfitting and Underfitting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[htp]
    \centering
    \input{graphics/tikz/overfitting}
%    \caption{Multilayer perceptron}
%    \label{fig:mlp}
\end{figure}

\begin{figure}[htp]
    \centering
    \input{graphics/tikz/capacity-vs-error}
    \captionsetup{format=hang} % hanging captions
    \caption{
        Typical relationship between capacity and error. Training and test error
        behave differently. At the left end of the graph, training error and
        generalization error are both high. This is the \textbf{underfitting
        regime} As we increase capacity, training error decreases, but the gap
        between training and generalisation error increases. Eventually, the
        size of this gap outweighs the decrease in training error, and we enter
        the \textbf{overfitting regime}, where capacity is too large, above the
        \textbf{optimal capacity}. Adapted from Goodfellow et. al.
        \cite{Goodfellow-et-al-2016}.
    }
    \label{fig:capacity}
\end{figure}


\subsubsection{No Free Lunch Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Regularization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Hyperparameters and Validation Sets}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Resubstitution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Holdout}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{K-Fold Cross-Validation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[htbp]
    \centering
    \input{graphics/tikz/kfold-cv}
    \captionsetup{format=hang} % hanging captions
    \caption{
        K-fold cross-validation procedure: (1) Dataset is divided into K-folds
        of roughly equal size. (2) Choose one fold randomly to be the
        holdout set then fit model on the remaining K-1 folds. (3)
        Iterate through the remaining K-1 folds, using each as the holdout set
        and record the error $e_i$ of the iteration. (4) Average the errors
        obtained over the K-folds.
    }
    \label{fig:kfold-cv}
\end{figure}

\subsubsection{Leave-One-Out Cross-Validation (LOOCV)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Bootstrapping}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Supervised}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Deep Learning\label{sec:DL}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Gls{DL} is a field of \gls{ML} that is primarily concerned with
the learning of representations of data. The main idea is to use a \gls{NN} to learn representations of data. The main difference between a
\gls{NN} and a traditional \gls{ML} algorithm is that the
neural network is a function of the data, rather than the data itself.

\subsection{The fundamental component: Perceptrons}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Proposed by Rosenblatt \cite{Rosenblatt_1957_6098} in his technical
report funded by the United States Office of Naval Research
\cite{doi:10.1177/030631296026003005} in 1957, the \textit{perceptron} is a
fundamental component of deep learning, describing a mathematical model of a
biological neuron. There are two different sets of notation which exist when
dealing with the bias of a perceptron. One involves the inclusion of a unit
constant in the input vector $\mathbf{x}$, with the bias being specified by the
value of the first weight $w_0$. An alternative notation exists which treats the
bias as a standalone value $b$. The latter notation will be used, as it is the
preferred notation in contemporary deep learning papers. The notation defining
the mapping of a perceptron is then: $f(\mathbf{x};\mathbf{w},
b)=\mathbf{x}^T\mathbf{w}+b$; $x\in\mathbb{R}^{d_{in}}$,
$\mathbf{w}\in\mathbb{R}^{d_{in}}$, $b\in\mathbb{R}$.

\begin{figure}[htbp]
    \centering
    \input{graphics/tikz/perceptron}
    \caption{Perceptron}
    \label{fig:perceptron}
\end{figure}

This mathematical mimicry of a biological neuron was first used used proposed by
Rosenblatt where a set of inputs $\mathbf{x}$ weighted using $\mathbf{w}$ before
being summed. If this sum surpassed a threshold, then a binary output of $1$ was
returned. \autoref{fig:perceptron} shows a more general formulation which
applies to the resulting field of deep learning, with any activation function
$\phi$ analogous for the level of excitation of a biological neuron in response
to its stimulus $\mathbf{x}$.

The primary criticism of the perceptron came in 1969 from Minsky and Papert
\cite{minsky69perceptrons}, where it was shown that the perceptron could only
solve \textit{linearly separable} functions, and failed to solve the XOR and
NXOR functions. They went on to claim that the research being done was doomed to
failure due to these limitations, resulting in little research in the area being
done until about the 1980's.

\subsection{Activation function}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Since the early days of the perceptron, a wide variety of activation functions
$\phi$ have been used and improve upon the threshold step function.

%\begin{figure}[htbp]
%    \centering
%    \input{graphics/tikz/activation}
%    \caption{Activation functions}
%    \label{fig:activation}
%\end{figure}
\input{graphics/tikz/activation}

\subsection{Multilayer perceptrons, a.k.a feed forward networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

$f(\mathbf{x};\mathbf{w}, b)=\mathbf{x}^T\mathbf{w}+b$; $x\in\mathbb{R}^{d_{in}}$, $\mathbf{w}\in\mathbb{R}^{d_{in}\times}$, $b\in\mathbb{R}^1$.

\begin{figure}
    \centering
    \input{graphics/tikz/mlp}
    \caption{Multilayer perceptron}
    \label{fig:mlp}
\end{figure}


\begin{figure}
    \centering
    \input{graphics/tikz/mlp-vec}
    \caption{Multilayer perceptron}
    \label{fig:mlp-vec}
\end{figure}


\begin{equation}
    \mathbf{h}^{(1)} = g^{(1)} \bigg(\mathbf{W}^{{(1)}\;T}\mathbf{x} + \mathbf{b}^{(1)}\bigg);
\end{equation}

\begin{equation}
    \mathbf{h}^{(2)} = g^{(2)} \bigg(\mathbf{W}^{{(2)}\;T}\mathbf{h}^{(1)} + \mathbf{b}^{(2)}\bigg);
\end{equation}

\subsection{Gradient based learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Universal Approximation Properties and Depth}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A linear model by definition, may only optimised to represent linear functions.
It has advantages in its simplicity to optimise however we often require our
estimator models to learn nonlinear functions.

\subsection{Backpropagation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Backpropagation, short for "backward propagation of errors," is a
\textit{supervised learning} algorithm for artificial neural networks using
\textit{gradient descent}.


\section{Reinforcement Learning\label{ssec:RL}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{graphics/tikz/agent-environment}

\begin{itemize}
    \item states and observations,
    \item action spaces,
    \item policies,
    \item trajectories,
    \item different formulations of return,
    \item the RL optimization problem,
    \item and value functions.
\end{itemize}

\subsection{Taxonomy of Reinforcement Learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Value-based methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Policy-based methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Policy gradient}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Deep deterministic policy gradient (DDPG)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
